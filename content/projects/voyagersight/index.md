---
draft: false
title: "VoyagerSight"
description: "Replication and extension study on Nvidiaâ€™s groundbreaking Voyager, exploring the effects of multimodal inputs on Minecraft LLM agents"
cover:
  image: cover.png
  caption: "TODO Temporary Cover"
ShowToc: false
# weight: 2
weight: 3
---

VoyagerSight is a research replication and extension of [NVIDIA's Voyager](https://voyager.minedojo.org/) paper, integrating multimodal perception capabilities into autonomous Minecraft agents. Powered by modern LLMs with faster inference and enhanced multimodal capabilities, this implementation tests the hypothesis that agents with multimodal perception will perform more effectively and also display diverse behavior and creativity.

**Technologies Used:**

- [Python](https://www.python.org/)
- [LangChain](https://langchain.com/)
- [ChromaDB](https://docs.trychroma.com/docs/overview/introduction)
- [OpenAI](https://openai.com/)
- [Pandas](https://pandas.pydata.org/)
- [Google Colab](https://colab.research.google.com/)

---

**Full project documentation, code, and paper coming soon!**

<!--
Key things to highlight when writing:

- Replicated a cutting edge study on LLM agents in complicated environments
- Applied modern, smarter, faster LLMs to the task
- Prompt engineered to optimize task completion
- Extended the study to include multimodal inputs, using images from the agents POV to guide the agent
- RAG for skill selection
- Conducted similar ablation studies to understand the effects of multimodal inputs
- LangChain experience
- Restful API experience
  ` -->
